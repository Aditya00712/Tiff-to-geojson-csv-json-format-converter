import os
import numpy as np
import pandas as pd
import rasterio
import json
from rasterio.warp import transform
from pyproj import Transformer
from pathlib import Path

def extract_elevation_data(tiff_file_path, output_file_path):
    """
    Analyze TIFF file and return metadata without creating output files.
    
    Args:
        tiff_file_path (str): Path to the input TIFF file
        output_file_path (str): Base path for output files (not used)
        
    Returns:
        dict: Metadata about the TIFF file
    """
    try:
        # Open the TIFF file
        with rasterio.open(tiff_file_path) as dataset:
            # Read the elevation data
            elevation_data = dataset.read(1)  # Read the first band
            
            # Get dimensions
            height, width = elevation_data.shape
            
            print(f"Analyzing {tiff_file_path}...")
            print(f"Dimensions: {width} x {height}")
            print(f"CRS: {dataset.crs}")
            print(f"Total pixels: {height * width}")
            print(f"NoData value: {dataset.nodata}")
            
            # Get elevation data statistics
            z_values = elevation_data.flatten()
            total_points = len(z_values)
            
            # Check if all elevation values are NoData/infinity
            nodata_threshold = -1e30  # Any value smaller than this is considered NoData
            valid_elevation_mask = z_values > nodata_threshold
            valid_elevation_count = np.sum(valid_elevation_mask)
            
            print(f"Valid elevation points: {valid_elevation_count:,} / {total_points:,}")
            print(f"Percentage valid: {(valid_elevation_count / total_points) * 100:.2f}%")
            
            # Calculate bounds for valid data only
            if valid_elevation_count > 0:
                # Get coordinates for valid points only
                rows, cols = np.meshgrid(np.arange(height), np.arange(width), indexing='ij')
                rows_flat = rows.flatten()[valid_elevation_mask]
                cols_flat = cols.flatten()[valid_elevation_mask]
                
                # Convert to world coordinates
                x_coords, y_coords = rasterio.transform.xy(dataset.transform, rows_flat, cols_flat)
                
                # Transform to WGS84
                transformer = Transformer.from_crs(dataset.crs, 'EPSG:4326', always_xy=True)
                lon_coords, lat_coords = transformer.transform(x_coords, y_coords)
                
                valid_z_values = z_values[valid_elevation_mask]
                
                bounds_wgs84 = {
                    'min_longitude': float(np.min(lon_coords)),
                    'max_longitude': float(np.max(lon_coords)),
                    'min_latitude': float(np.min(lat_coords)),
                    'max_latitude': float(np.max(lat_coords)),
                    'min_elevation': float(np.min(valid_z_values)),
                    'max_elevation': float(np.max(valid_z_values))
                }
            else:
                bounds_wgs84 = None
            
            # Return metadata
            metadata = {
                'source_file': Path(tiff_file_path).name,
                'file_path': str(tiff_file_path),
                'dimensions': [width, height],
                'total_pixels': total_points,
                'valid_pixels': int(valid_elevation_count),
                'valid_percentage': (valid_elevation_count / total_points) * 100,
                'original_crs': str(dataset.crs),
                'nodata_value': dataset.nodata,
                'bounds_wgs84': bounds_wgs84,
                'has_valid_data': valid_elevation_count > 0,
                'data_sparsity': 'sparse' if (valid_elevation_count / total_points) < 0.1 else 'dense'
            }
            
            print(f"Analysis complete: {valid_elevation_count:,} valid points")
            return metadata
            
    except Exception as e:
        print(f"Error analyzing {tiff_file_path}: {str(e)}")
        return {
            'source_file': Path(tiff_file_path).name,
            'file_path': str(tiff_file_path),
            'error': str(e),
            'has_valid_data': False
        }

def process_all_tiff_files(input_directory, output_directory):
    """
    Analyze all TIFF files and collect metadata without creating output files.
    
    Args:
        input_directory (str): Directory containing TIFF files
        output_directory (str): Directory to save master README
        
    Returns:
        list: List of metadata dictionaries for all TIFF files
    """
    # Create output directory if it doesn't exist
    Path(output_directory).mkdir(exist_ok=True)
    
    # Find all TIFF files
    tiff_files = []
    for ext in ['*.tif', '*.tiff', '*.TIF', '*.TIFF', '*.tiff.tiff']:
        tiff_files.extend(Path(input_directory).glob(ext))
    
    # Remove duplicates by converting to set and back to list
    tiff_files = list(set(tiff_files))
    tiff_files.sort()  # Sort for consistent ordering
    
    if not tiff_files:
        print(f"No TIFF files found in {input_directory}")
        return []
    
    print(f"Found {len(tiff_files)} TIFF files")
    
    all_metadata = []
    total_valid_points = 0
    
    for tiff_file in tiff_files:
        # Analyze the TIFF file
        metadata = extract_elevation_data(str(tiff_file), "")
        all_metadata.append(metadata)
        
        if metadata.get('has_valid_data', False):
            total_valid_points += metadata.get('valid_pixels', 0)
        
        print(f"Completed: {tiff_file.name}")
        print("-" * 50)
    
    print(f"\nAnalysis complete!")
    print(f"Total VALID data points across all files: {total_valid_points:,}")
    print(f"Files with valid data: {sum(1 for m in all_metadata if m.get('has_valid_data', False))}/{len(all_metadata)}")
    
    return all_metadata

def analyze_tiff_file(tiff_file_path):
    """
    Analyze a TIFF file to understand its properties and data distribution.
    
    Args:
        tiff_file_path (str): Path to the TIFF file
    """
    try:
        with rasterio.open(tiff_file_path) as dataset:
            print(f"\n=== Analysis of {tiff_file_path} ===")
            print(f"Dimensions: {dataset.width} x {dataset.height}")
            print(f"Number of bands: {dataset.count}")
            print(f"Data type: {dataset.dtypes}")
            print(f"CRS: {dataset.crs}")
            print(f"NoData value: {dataset.nodata}")
            print(f"Transform: {dataset.transform}")
            
            # Read the data
            data = dataset.read(1)
            
            # Basic statistics
            print(f"\nData Statistics:")
            print(f"  Shape: {data.shape}")
            print(f"  Min value: {data.min()}")
            print(f"  Max value: {data.max()}")
            print(f"  Mean value: {np.mean(data)}")
            
            # Check for valid data
            if dataset.nodata is not None:
                valid_mask = data != dataset.nodata
                valid_count = np.sum(valid_mask)
                print(f"  Valid pixels: {valid_count} / {data.size}")
                print(f"  Percentage valid: {(valid_count / data.size) * 100:.2f}%")
                
                if valid_count > 0:
                    valid_data = data[valid_mask]
                    print(f"  Valid data range: {valid_data.min()} to {valid_data.max()}")
                    print(f"  Valid data mean: {valid_data.mean()}")
            
            # Sample some pixels
            print(f"\nSample pixel values (first 5x5):")
            sample_size = min(5, data.shape[0]), min(5, data.shape[1])
            for i in range(sample_size[0]):
                row_values = []
                for j in range(sample_size[1]):
                    row_values.append(f"{data[i,j]:.2e}")
                print(f"  Row {i}: {' '.join(row_values)}")
            
    except Exception as e:
        print(f"Error analyzing {tiff_file_path}: {str(e)}")

def create_combined_file(output_directory):
    """
    Combine all individual elevation data files into one master file.
    
    Args:
        output_directory (str): Directory containing individual text files
    """
    # Find all elevation data files
    data_files = list(Path(output_directory).glob("*_elevation_data.txt"))
    
    if not data_files:
        print("No elevation data files found to combine")
        return
    
    print(f"Combining {len(data_files)} files...")
    
    # Read and combine all files
    combined_df = pd.DataFrame()
    
    for file_path in data_files:
        df = pd.read_csv(file_path)
        # Add a column to identify the source file
        df['source_file'] = file_path.stem.replace('_elevation_data', '')
        combined_df = pd.concat([combined_df, df], ignore_index=True)
    
    # Save combined file
    combined_path = Path(output_directory) / "combined_elevation_data.txt"
    combined_df.to_csv(combined_path, index=False, sep=',')
    
    print(f"Combined file saved: {combined_path}")
    print(f"Total combined data points: {len(combined_df)}")

def create_master_readme(output_directory, all_metadata):
    """
    Create a comprehensive master README file with analysis of all TIFF files.
    
    Args:
        output_directory (str): Directory to save the master README
        all_metadata (list): List of metadata dictionaries from all TIFF files
    """
    master_readme_path = Path(output_directory) / "MASTER_TIFF_ANALYSIS.txt"
    
    # Calculate overall statistics
    valid_files = [m for m in all_metadata if m.get('has_valid_data', False)]
    invalid_files = [m for m in all_metadata if not m.get('has_valid_data', False)]
    
    total_files = len(all_metadata)
    total_valid_points = sum(m.get('valid_pixels', 0) for m in valid_files)
    total_pixels = sum(m.get('total_pixels', 0) for m in all_metadata if 'total_pixels' in m)
    
    # Calculate overall bounds
    overall_bounds = None
    if valid_files:
        all_bounds = [m['bounds_wgs84'] for m in valid_files if m.get('bounds_wgs84')]
        if all_bounds:
            overall_bounds = {
                'min_longitude': min(b['min_longitude'] for b in all_bounds),
                'max_longitude': max(b['max_longitude'] for b in all_bounds),
                'min_latitude': min(b['min_latitude'] for b in all_bounds),
                'max_latitude': max(b['max_latitude'] for b in all_bounds),
                'min_elevation': min(b['min_elevation'] for b in all_bounds),
                'max_elevation': max(b['max_elevation'] for b in all_bounds)
            }
    
    with open(master_readme_path, 'w') as f:
        f.write("MASTER TIFF ELEVATION DATA ANALYSIS\n")
        f.write("=" * 70 + "\n\n")
        f.write(f"Analysis completed: {pd.Timestamp.now()}\n")
        f.write(f"Input directory: {Path(input_dir).absolute()}\n\n")
        
        # Summary Statistics
        f.write("SUMMARY STATISTICS\n")
        f.write("-" * 50 + "\n")
        f.write(f"Total TIFF files analyzed: {total_files}\n")
        f.write(f"Files with valid data: {len(valid_files)}\n")
        f.write(f"Files with no valid data: {len(invalid_files)}\n")
        f.write(f"Success rate: {(len(valid_files)/total_files)*100:.1f}%\n\n")
        
        f.write(f"Total pixels across all files: {total_pixels:,}\n")
        f.write(f"Total valid elevation points: {total_valid_points:,}\n")
        if total_pixels > 0:
            f.write(f"Overall data density: {(total_valid_points/total_pixels)*100:.2f}%\n\n")
        
        # Overall Coverage
        if overall_bounds:
            f.write("OVERALL GEOGRAPHIC COVERAGE (WGS84)\n")
            f.write("-" * 50 + "\n")
            f.write(f"Longitude range: {overall_bounds['min_longitude']:.6f} to {overall_bounds['max_longitude']:.6f}\n")
            f.write(f"Latitude range: {overall_bounds['min_latitude']:.6f} to {overall_bounds['max_latitude']:.6f}\n")
            f.write(f"Elevation range: {overall_bounds['min_elevation']:.2f} to {overall_bounds['max_elevation']:.2f} meters\n\n")
        
        # Files with Valid Data
        if valid_files:
            f.write("FILES WITH VALID ELEVATION DATA\n")
            f.write("=" * 70 + "\n")
            for i, metadata in enumerate(valid_files, 1):
                f.write(f"\n{i}. {metadata['source_file']}\n")
                f.write(f"   Dimensions: {metadata['dimensions'][0]} x {metadata['dimensions'][1]}\n")
                f.write(f"   Total pixels: {metadata['total_pixels']:,}\n")
                f.write(f"   Valid pixels: {metadata['valid_pixels']:,}\n")
                f.write(f"   Data density: {metadata['valid_percentage']:.2f}%\n")
                f.write(f"   Data type: {metadata['data_sparsity']}\n")
                f.write(f"   Original CRS: {metadata['original_crs']}\n")
                
                if metadata.get('bounds_wgs84'):
                    bounds = metadata['bounds_wgs84']
                    f.write(f"   Coverage (WGS84):\n")
                    f.write(f"     Longitude: {bounds['min_longitude']:.6f} to {bounds['max_longitude']:.6f}\n")
                    f.write(f"     Latitude: {bounds['min_latitude']:.6f} to {bounds['max_latitude']:.6f}\n")
                    f.write(f"     Elevation: {bounds['min_elevation']:.2f} to {bounds['max_elevation']:.2f} m\n")
                
                # Estimate potential file sizes
                points = metadata['valid_pixels']
                estimated_geojson_mb = (points * 0.1) / 1000000  # Rough estimate
                f.write(f"   Estimated GeoJSON size: {estimated_geojson_mb:.2f} MB\n")
                
                if metadata['valid_percentage'] < 1.0:
                    f.write(f"   NOTE: Sparse data - only {metadata['valid_percentage']:.2f}% valid pixels\n")
                elif metadata['valid_percentage'] > 90:
                    f.write(f"   NOTE: Dense data - {metadata['valid_percentage']:.2f}% valid pixels\n")
        
        # Files with No Valid Data
        if invalid_files:
            f.write(f"\n\nFILES WITH NO VALID ELEVATION DATA\n")
            f.write("=" * 70 + "\n")
            for i, metadata in enumerate(invalid_files, 1):
                f.write(f"\n{i}. {metadata['source_file']}\n")
                if 'error' in metadata:
                    f.write(f"   ERROR: {metadata['error']}\n")
                else:
                    f.write(f"   WARNING: All pixels contain NoData/infinity values\n")
                    if 'dimensions' in metadata:
                        f.write(f"   Dimensions: {metadata['dimensions'][0]} x {metadata['dimensions'][1]}\n")
                    if 'nodata_value' in metadata:
                        f.write(f"   NoData value: {metadata['nodata_value']}\n")
        
        # # Recommendations
        # f.write(f"\n\nRECOMMENDATIONS\n")
        # f.write("=" * 70 + "\n")
        
        # if len(valid_files) > 0:
        #     f.write("For files with valid data:\n")
        #     f.write("• Use them to create GeoJSON files for web mapping\n")
        #     f.write("• Sparse files will create small, efficient GeoJSON files\n")
        #     f.write("• Dense files may create large files - consider sampling if needed\n\n")
        
        # if len(invalid_files) > 0:
        #     f.write("For files with no valid data:\n")
        #     f.write("• Check if TIFF files are corrupted or incomplete\n")
        #     f.write("• Verify that elevation data exists in the files\n")
        #     f.write("• Consider different NoData threshold if needed\n\n")
        
        # Processing suggestions
        sparse_files = [m for m in valid_files if m.get('valid_percentage', 0) < 10]
        dense_files = [m for m in valid_files if m.get('valid_percentage', 0) > 50]
        
        if sparse_files:
            f.write(f"SPARSE DATA FILES ({len(sparse_files)} files < 10% valid):\n")
            f.write("• Very efficient for web mapping (small file sizes)\n")
            f.write("• Process all valid points without sampling\n")
            for m in sparse_files:
                f.write(f"  - {m['source_file']}: {m['valid_percentage']:.2f}% valid\n")
            f.write("\n")
        
        if dense_files:
            f.write(f"DENSE DATA FILES ({len(dense_files)} files > 50% valid):\n")
            f.write("• May create large GeoJSON files\n")
            f.write("• Consider sampling for web applications\n")
            for m in dense_files:
                f.write(f"  - {m['source_file']}: {m['valid_percentage']:.2f}% valid ({m['valid_pixels']:,} points)\n")
            f.write("\n")
        
        # f.write("NEXT STEPS:\n")
        # f.write("• Modify script to process only files with valid data\n")
        # f.write("• Use this analysis to decide on sampling strategies\n")
        # f.write("• Create GeoJSON files only for files with sufficient valid data\n")
    
    print(f"Master analysis created: {master_readme_path}")
    print(f"Summary: {len(valid_files)}/{total_files} files have valid elevation data")
    print(f"Total valid points: {total_valid_points:,}")
    
    return master_readme_path

if __name__ == "__main__":
    # Configuration
    input_dir = "tiffData"  # Directory containing TIFF files
    output_dir = "elevation_output"  # Directory to save master README
    
    print("=== TIFF File Analysis (No File Creation) ===")
    # First, let's analyze the TIFF files to understand their content
    tiff_files = []
    for ext in ['*.tif', '*.tiff', '*.TIF', '*.TIFF']:
        tiff_files.extend(Path(input_dir).glob(ext))
    
    # Remove duplicates and sort
    tiff_files = list(set(tiff_files))
    tiff_files.sort()
    
    print(f"Found {len(tiff_files)} TIFF files")
    print("Analysis strategy:")
    print("  - Analyze all TIFF files for elevation data")
    print("  - Check data density and coverage")
    print("  - NO files will be created during analysis")
    print("  - Only a master analysis report will be generated")
    print("  - Use report to decide which files to process later")
    
    for tiff_file in tiff_files[:2]:  # Analyze first 2 files for preview
        analyze_tiff_file(str(tiff_file))
    
    print("\n" + "="*60)
    print("=== Analyzing All TIFF Files ===")
    
    # Analyze all TIFF files without creating output files
    all_metadata = process_all_tiff_files(input_dir, output_dir)
    
    # Create comprehensive master README
    print("\n" + "="*60)
    print("=== Creating Master Analysis Report ===")
    create_master_readme(output_dir, all_metadata)
    
    print(f"\nAnalysis complete! Check the master report for detailed insights.")
    print(f"Report location: {output_dir}/MASTER_TIFF_ANALYSIS.txt")
    print(f"Use this analysis to decide which files are worth processing into GeoJSON format.")